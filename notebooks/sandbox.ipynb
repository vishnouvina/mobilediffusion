{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/27/2024 14:28:20 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n",
      "{'rescale_betas_zero_snr', 'variance_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "{'force_upcast', 'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "{'addition_time_embed_dim', 'encoder_hid_dim_type', 'dropout', 'attention_type', 'transformer_layers_per_block', 'num_attention_heads', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "{'addition_time_embed_dim', 'encoder_hid_dim_type', 'dropout', 'attention_type', 'transformer_layers_per_block', 'num_attention_heads', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "Resolving data files: 100%|█████████████████████| 24/24 [00:00<00:00, 45.73it/s]\n",
      "Loading dataset shards: 100%|█████████████████| 23/23 [00:00<00:00, 9065.78it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvishnou-vinayagame\u001b[0m (\u001b[33mvishnou\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/w/340/vishnouvina/wandb/run-20240327_142827-fpev44gs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mnorthern-breeze-47\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vishnou/text2image-fine-tune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vishnou/text2image-fine-tune/runs/fpev44gs/workspace\u001b[0m\n",
      "03/27/2024 14:28:30 - INFO - __main__ - ***** Running training *****\n",
      "03/27/2024 14:28:30 - INFO - __main__ -   Num examples = 20072\n",
      "03/27/2024 14:28:30 - INFO - __main__ -   Num Epochs = 32\n",
      "03/27/2024 14:28:30 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "03/27/2024 14:28:30 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "03/27/2024 14:28:30 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "03/27/2024 14:28:30 - INFO - __main__ -   Total optimization steps = 10000\n",
      "Steps:   0%|       | 12/10000 [01:11<16:03:21,  5.79s/it, lr=1e-5, step_loss=40]/w/340/vishnouvina/miniconda3/envs/mdenv/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Steps:   1%|     | 76/10000 [07:13<15:50:14,  5.75s/it, lr=1e-5, step_loss=38.5]"
     ]
    }
   ],
   "source": [
    "!accelerate launch --mixed_precision=\"fp16\" /w/340/vishnouvina/mobilediffusion/lora_distill_training.py \\\n",
    "    --pretrained_model_name_or_path=\"SG161222/Realistic_Vision_V4.0\" \\\n",
    "    --dataset_name=\"fantasyfish/laion-art\" \\\n",
    "    --caption_column=\"text\"\\\n",
    "    --resolution=512 --center_crop --random_flip \\\n",
    "    --num_train_epochs=5 \\\n",
    "    --gradient_accumulation_steps=4 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --max_train_steps=10000 \\\n",
    "    --validation_epochs=1 \\\n",
    "    --distill_level=\"sd_tiny\"\\\n",
    "    --prepare_unet=\"True\"\\\n",
    "    --use_peft\\\n",
    "    --use_8bit_adam\\\n",
    "    --mixed_precision=\"bf16\"\\\n",
    "    --output_weight=0.5\\\n",
    "    --feature_weight=0.5\\\n",
    "    --learning_rate=1e-05 \\\n",
    "    --max_grad_norm=1 \\\n",
    "    --lr_scheduler=\"constant\"\\\n",
    "    --lr_warmup_steps=0 \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --lora_r=4 \\\n",
    "    --lora_alpha=32 \\\n",
    "    --push_to_hub \\\n",
    "    --hub_model_id=\"sd-laion-art\"\\\n",
    "    --output_dir=\"sd-laion-art\"\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 100, 25)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "max_train_steps = 100\n",
    "len_train_dataloader = 100\n",
    "num_train_epochs = 5\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len_train_dataloader / gradient_accumulation_steps)\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len_train_dataloader / gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "num_train_epochs,max_train_steps, num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#srun --partition biggpunodes --nodelist gpunode1 -c 4 --pty --gres=gpu:1 --mem=48G  accelerate launch --mixed_precision=\"fp16\"  lora_distill_training.py --pretrained_model_name_or_path=\"SG161222/Realistic_Vision_V4.0\" --dataset_name=\"fantasyfish/laion-art\" --caption_column=text --resolution=512 --center_crop --random_flip --train_batch_size=1 --num_train_epochs=6 --gradient_accumulation_steps=4 --gradient_checkpointing --max_train_steps=5000 --distill_level=sd_tiny --prepare_unet=True --use_peft --output_weight=0.5 --feature_weight=0.5 --learning_rate=1e-05 --max_grad_norm=1 --lr_scheduler=constant --lr_warmup_steps=0 --validation_prompts= [\"A man in a suit\"]  --report_to=wandb --lora_r=4 --lora_alpha=32 --push_to_hub --hub_model_id=laion-art --output_dir=sd-laion-art"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mdenv)",
   "language": "python",
   "name": "mdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
